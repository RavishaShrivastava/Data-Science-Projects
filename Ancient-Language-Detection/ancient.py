# -*- coding: utf-8 -*-
"""Ancient.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B-WuhZ53CXs6h103ntWtX_CDMidMSYeK
"""

# #Uploading the zip file
# from google.colab import files
# uploaded = files.upload()

# #Unzipping the file
# !unzip -q ancient_lang.zip

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt

#Data path
TRAIN_DIR = "ancient_lang/Ancient/train"
VALID_DIR = "ancient_lang/Ancient/valid"
TEST_DIR  = "ancient_lang/Ancient/test"

print("**"*60)
print("Laoded successfully")

IMG_SIZE = (224,224)
BATCH_SIZE = 5

train_ds = tf.keras.utils.image_dataset_from_directory(
    TRAIN_DIR,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int"
)

valid_ds = tf.keras.utils.image_dataset_from_directory(
    VALID_DIR,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int"
)
test_ds = tf.keras.utils.image_dataset_from_directory(
    TEST_DIR,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="int",
    shuffle=False
)


class_names = train_ds.class_names
print("Classes:", class_names)
print("Number of classes:", len(class_names))
print("Test classes:", test_ds.class_names)

plt.figure(figsize=(8, 8))

for images, labels in train_ds.take(1):
    for i in range(min(6, len(images))):
        ax = plt.subplot(2, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")

plt.show()

from tensorflow import keras
from keras import layers
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.05),
    layers.RandomZoom(0.05),
    layers.RandomTranslation(0.1, 0.1),
])

normalization_layer = layers.Rescaling(1./255)
print("DONE NORMAL")
train_ds = train_ds.map(
    lambda x, y: (normalization_layer(data_augmentation(x)), y)
)

valid_ds = valid_ds.map(
    lambda x, y: (normalization_layer(x), y)
)

test_ds = test_ds.map(
    lambda x, y: (normalization_layer(x), y)
)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.shuffle(100).prefetch(AUTOTUNE)
valid_ds = valid_ds.cache().prefetch(AUTOTUNE)
test_ds  = test_ds.cache().prefetch(AUTOTUNE)

for images, labels in train_ds.take(1):
    for i in range(min(4,len(images))):
        ax = plt.subplot(2, 3, i + 1)
        plt.imshow(images[i])
        plt.title(class_names[labels[i]])
        plt.axis("off")

plt.show()

base_model = MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights="imagenet"
)

base_model.trainable = False  # VERY IMPORTANT

num_classes = len(class_names)

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(128, activation="relu"),
    Dropout(0.5),
    Dense(num_classes, activation="softmax")
])
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)
model.summary()

EPOCHS = 15

history = model.fit(
    train_ds,
    validation_data=valid_ds,
    epochs=EPOCHS
)
plt.figure(figsize=(12, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Accuracy")

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Loss")

plt.show()



early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

checkpoint = ModelCheckpoint(
    'best_model.keras',
    monitor='val_loss',
    save_best_only=True,
    mode='min'
)

history = model.fit(
    train_ds,
    validation_data=valid_ds,
    epochs=EPOCHS,
    callbacks=[early_stop, checkpoint]
)


plt.figure(figsize=(12, 4))
# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Accuracy")
# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Loss")
plt.show()
try:
  test_loss, test_acc = model.evaluate(test_ds)
  print(f"Test Accuracy: {test_acc*100:.2f}%")
  predictions = model.predict(test_ds)
  pred_labels = tf.argmax(predictions, axis=1)
  plt.figure(figsize=(12, 8))
  for i, (images, labels) in enumerate(test_ds.take(5)):
      for j in range(min(BATCH_SIZE, len(images))):
          ax = plt.subplot(2, BATCH_SIZE, i*BATCH_SIZE + j + 1)
          plt.imshow(images[j].numpy())
          plt.title(f"True: {class_names[labels[j]]}\nPred: {class_names[pred_labels[i*BATCH_SIZE + j]]}")
          plt.axis("off")
  plt.show()
except:
  print("Error")



